.. skbel documentation master file, created by
   sphinx-quickstart on Sat May 29 18:15:24 2021.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

SKBEL
=================================

.. currentmodule:: SKBEL

This package implements the Bayesian Evidential Learning framework.

Installation
------------

SKBEL is available through `PyPI <https://pypi.org/project/skbel/>`_, and may be installed using ``pip``: ::

   $ pip install skbel

Contents
-----------------

.. toctree::
   :maxdepth: 2

   source/modules.rst
   source/examples.rst

Bayesian Evidential Learning
----------------------------
Introduction
.............

.. image:: /img/evidential.png
   :width: 800


Figure 1: The concept of BEL. d = predictor (observed data), h = target (parameter of interest), m = model.

- The idea of BEL is to find a direct relationship between `d` (predictor) and `h` (target) in a reduced dimensional space with machine learning.
- Both `d` and `h` are generated by forward modelling from the same set of prior models `m`.
- Given a new measured predictor `d*`, this relationship is used to infer the posterior probability distribution of the target, without the need for a computationally expensive inversion.
- The posterior distribution of the target is then sampled and backtransformed from the reduced dimensional space to the original space to predict posterior realizations of `h` given `d*`.

Workflow
.........
Forward modeling
++++++++++++++++
- Examples of both `d` and `h` are generated through forward modeling from the same model `m`. Target and predictor are real, multi-dimensional random variables.

Pre-processing
++++++++++++++++

- Specific pre-processing is applied to the data if necessary (such as scaling).

Dimensionality reduction
+++++++++++++++++++++++++

- Principal Component Analysis (PCA) is applied to both target and predictor to aggregate the correlated variables into a few independent Principal Components (PC’s).

Learning
++++++++++++++++

- Canonical Correlation Analysis (CCA) transforms the two sets into pairs of Canonical Variates (CV’s) independent of each other.

Post-processing
++++++++++++++++

- Specific post-processing is applied to the CV's if necessary (such as CV normalization).

Posterior distribution inference
+++++++++++++++++++++++++++++++++++

- The mean `μ` and covariance `Σ` of the posterior distribution of an unknown target given an observed `d*` can be directly estimated from the CV's distribution.
- Alternatively, the posterior conditional distribution can be inferred through KDE.

Sampling and back-transformation to the original space
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

- The posterior distribution is sampled to obtain realizations of `h` in canonical space, successively back-transformed to the original space.

.. image:: /img/flow-01.png
   :width: 800

Figure 2: Typical BEL workflow.

Contributing
=============
Contributors and feedback from users are welcome. Don't hesitate to submit an issue or a PR, or request a new feature.

References
=============

.. bibliography::
   :all:


Issues and contributing
-----------------------

Contributors and feedback from users are welcome. Don't hesitate to submit an issue or a PR, or request a new feature.

Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`
